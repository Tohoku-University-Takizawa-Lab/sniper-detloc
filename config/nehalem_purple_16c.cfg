# Configuration file for Xeon X5550 Gainestown
# See http://en.wikipedia.org/wiki/Gainestown_(microprocessor)#Gainestown
# and http://ark.intel.com/products/37106

#include nehalem

# Xeon processor in purple machine
[perf_model/core]
frequency = 2.4
#logical_cpus = 2

#[perf_model/dtlb]
#size = 256             # Number of D-TLB entries
#associativity = 4     # D-TLB associativity

[perf_model/l1_icache]
perfect = false
cache_size = 32
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 4
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
shared_cores = 2

[perf_model/l1_dcache]
perfect = false
cache_size = 32
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 4
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
#shared_cores = 1
shared_cores = 2

[perf_model/l2_cache]
perfect = false
cache_size = 256   # should be power of 2, close to 2560
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 8 # 8.something according to membench, -1 cycle L1 tag access time
# http://www.realworldtech.com/page.cfm?ArticleID=RWT040208182719&p=7
tags_access_time = 3
# Total neighbor L1/L2 access time is around 40/70 cycles (60-70 when it's coming out of L1)
writeback_time = 50 # L3 hit time will be added
perf_model_type = parallel
writethrough = 0
#shared_cores = 1
shared_cores = 2

# Cache size must be in power of two, then
# it can divided perfectly by the previous cache size
# and then it can be divided perfectly by (assoc * block_size)
[perf_model/l3_cache]
perfect = false
cache_block_size = 64
#cache_size = 8192 
cache_size = 16384  # close to 25600
associativity = 16
address_hash = mask
replacement_policy = lru
data_access_time = 30 # 35 cycles total according to membench, +L1+L2 tag times
tags_access_time = 10
perf_model_type = parallel
writethrough = 0
shared_cores = 16

[perf_model/dram_directory]
# total_entries = number of entries per directory controller.
total_entries = 1048576
associativity = 32
directory_type = full_map
trace_latency = false

[perf_model/dram]
# -1 means that we have a number of distributed DRAM controllers (4 in this case)
#num_controllers = -1
num_controllers = -1
#controllers_interleaving = 14
controllers_interleaving = 16
# DRAM access latency in nanoseconds. Should not include L1-LLC tag access time, directory access time (14 cycles = 5.2 ns),
# or network time [(cache line size + 2*{overhead=40}) / network bandwidth = 18 ns]
# Membench says 175 cycles @ 2.66 GHz = 66 ns total
latency = 45
# Intel mlc says 80-82 ns local, 128 remote
# net time 64 + 80 / 32 =
# 80 - 23.2
#latency = 56
#latency = 64
#From measurement in intel forum, purple (E5 2640 v4) 35.76 GB/s (dual), 17.88 (single) per socket
#From spec, 68.3 GB/s, 4 channel
#Stream triad 82 GB/s
per_controller_bandwidth = 35.76             # default purple dual (interleaving means 1 ctrl per total_interleaving (1 socket)
#per_controller_bandwidth = 4.5              # per 2 to simulate the increase of size to one class higher
chips_per_dimm = 4
dimms_per_controller = 4

[network]
memory_model_1 = bus
memory_model_2 = bus

[network/bus]
bandwidth = 25.6 # in GB/s. Actually, it's 12.8 GB/s per direction and per connected chip pair
#bandwidth = 32 # 8 GT/s QPIignore_local_traffic = true # Memory controllers are on-chip, so traffic from core0 to dram0 does not use the QPI links
#bandwidth = 8 # 8 GT/s QPIignore_local_traffic = true # Memory controllers are on-chip, so traffic from core0 to dram0 does not use the QPI links
ignore_local_traffic = true # Memory controllers are on-chip, so traffic from core0 to dram0 does not use the QPI links

#[scheduler]
#type = static_map

#[scheduler/static]
#map_file = /home/agung/sniper-mapping/32c_packed.map

#[comm_tracer]
#enable = false
#comm_size = 6
#producer_consumer_mode = true

#[traceinput]
#address_randomization = false
